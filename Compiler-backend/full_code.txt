# api_server/main.py

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import subprocess
import os
import base64
import json

app = FastAPI()

origins = [
    "http://localhost:5173",
    "http://localhost:3000",
    "http://localhost:8080",
    # Add any other origins you expect
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def parse_gcc_errors(stderr_output: str):
    """
    Parses GCC stderr output to extract error details.
    """
    errors = []
    for line in stderr_output.split('\n'):
        if line.strip() == "":
            continue
        # Example GCC error format:
        # path/to/file.c:line:column: error: message
        parts = line.split(':')
        if len(parts) < 4:
            continue
        try:
            file_path = parts[0]
            line_num = int(parts[1])
            column_num = int(parts[2])
            error_type = parts[3].strip().split(' ')[0]  # e.g., 'error'
            message = ':'.join(parts[3:]).strip()
            errors.append({
                "type": error_type.capitalize(),
                "message": message,
                "line": line_num,
                "column": column_num
            })
        except ValueError:
            continue
    return errors

def generate_ast_image():
    """
    Placeholder function to generate AST image.
    Replace with actual AST generation logic.
    """
    # For demonstration, returning None
    return None

@app.post("/compile")
async def compile_code(request: dict):
    try:
        code = request.get("code", "")
        if not code:
            raise ValueError("No code provided.")

        # Save the code to a temporary file
        os.makedirs("data", exist_ok=True)
        code_path = "data/user_code.c"
        with open(code_path, "w") as f:
            f.write(code)
        
        # Compile the code using gcc
        compile_process = subprocess.run(
            ["gcc", code_path, "-o", "data/user_program"],
            capture_output=True,
            text=True
        )

        if compile_process.returncode != 0:
            # Compilation failed, parse errors
            error_output = compile_process.stderr
            errors = parse_gcc_errors(error_output)
            return {
                "success": False,
                "errors": errors,
                "symbol_table": {},
                "ast_image": None
            }
        
        # If compilation is successful, proceed to lexical and syntax analysis
        # Placeholder for actual analysis
        symbol_table = {}  # Populate based on analysis
        ast_image = generate_ast_image()  # Generate AST image if applicable

        return {
            "success": True,
            "errors": [],
            "symbol_table": symbol_table,
            "ast_image": ast_image
        }
    
    except Exception as e:
        print(f"Error during compilation: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
# api_server/routes.py

from fastapi import APIRouter, HTTPException
from .schemas import CompileRequest, CompileError, CompileResponse
import subprocess
import os
import uuid
import json
import base64

router = APIRouter()

@router.post("/compile", response_model=CompileResponse)
def compile_code(request: CompileRequest):
    """
    Compiles the submitted C code and returns the compilation results.
    """
    # Generate a unique filename for the submission
    unique_id = uuid.uuid4().hex
    code_filename = f"data/sample_c_codes/user_code_{unique_id}.c"

    # Save the submitted code to the unique file
    try:
        with open(code_filename, 'w') as f:
            f.write(request.code)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save code: {str(e)}")

    # Define the output file
    output_file = "full_code.txt"

    # Run the compilation script
    try:
        # Ensure the script has execution permissions
        os.chmod("scripts/code.sh", 0o755)

        # Execute the compilation script
        subprocess.run(["./scripts/code.sh"], check=True)
    except subprocess.CalledProcessError as e:
        raise HTTPException(status_code=500, detail="Compilation process failed.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error running compilation script: {str(e)}")

    # Read compilation errors
    errors_path = "data/errors.json"
    if os.path.exists(errors_path):
        try:
            with open(errors_path, 'r') as f:
                errors = json.load(f)
        except Exception as e:
            errors = [{"type": "System", "message": f"Failed to read errors.json: {str(e)}", "line": 0, "column": 0}]
    else:
        errors = []

    # Read symbol table
    symbol_table_path = "output/symbol_table.json"
    if os.path.exists(symbol_table_path):
        try:
            with open(symbol_table_path, 'r') as f:
                symbol_table = json.load(f)
        except Exception as e:
            symbol_table = {}
    else:
        symbol_table = {}

    # Read AST image and encode it in Base64
    ast_image_path = "ast_output.png"
    if os.path.exists(ast_image_path):
        try:
            with open(ast_image_path, 'rb') as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
            ast_image = f"data:image/png;base64,{encoded_string}"
        except Exception as e:
            ast_image = None
    else:
        ast_image = None

    # Determine success based on presence of errors
    success = len(errors) == 0

    # Cleanup: Remove the user-submitted code file
    try:
        os.remove(code_filename)
    except Exception as e:
        pass  # Optionally, log this exception

    return CompileResponse(
        success=success,
        errors=errors,
        symbol_table=symbol_table,
        ast_image=ast_image
    )
# api_server/schemas.py

from pydantic import BaseModel
from typing import List, Optional

class CompileRequest(BaseModel):
    code: str

class CompileError(BaseModel):
    type: str
    message: str
    line: int
    column: int

class CompileResponse(BaseModel):
    success: bool
    errors: List[CompileError]
    symbol_table: dict
    ast_image: Optional[str]  # Base64-encoded image string
#!/bin/bash

# Define the directory where your project is located
PROJECT_DIR="/mnt/c/Users/chait/OneDrive/Documents/Projects/Compiler/trial3/back"

# Define the output file
OUTPUT_FILE="full_code.txt"

# Navigate to the project directory
cd "$PROJECT_DIR" || exit

# Find and concatenate all code files into the output file
find . \( -name "*.py" -o -name "*.c" -o -name "*.sh" \) -exec cat {} + > "$OUTPUT_FILE"

# Perform lexical analysis and syntax analysis
python3 src/main.py "$OUTPUT_FILE"

# Confirm completion
echo "All code has been concatenated and analyzed into $OUTPUT_FILE"
# 0 "data/user_code.c"
# 0 "<built-in>"
# 0 "<command-line>"
# 1 "/usr/include/stdc-predef.h" 1 3 4
# 0 "<command-line>" 2
# 1 "data/user_code.c"
# 1 "/usr/include/stdio.h" 1 3 4
# 27 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/libc-header-start.h" 1 3 4
# 33 "/usr/include/x86_64-linux-gnu/bits/libc-header-start.h" 3 4
# 1 "/usr/include/features.h" 1 3 4
# 392 "/usr/include/features.h" 3 4
# 1 "/usr/include/features-time64.h" 1 3 4
# 20 "/usr/include/features-time64.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wordsize.h" 1 3 4
# 21 "/usr/include/features-time64.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 1 3 4
# 19 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wordsize.h" 1 3 4
# 20 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 2 3 4
# 22 "/usr/include/features-time64.h" 2 3 4
# 393 "/usr/include/features.h" 2 3 4
# 486 "/usr/include/features.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/sys/cdefs.h" 1 3 4
# 559 "/usr/include/x86_64-linux-gnu/sys/cdefs.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wordsize.h" 1 3 4
# 560 "/usr/include/x86_64-linux-gnu/sys/cdefs.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/long-double.h" 1 3 4
# 561 "/usr/include/x86_64-linux-gnu/sys/cdefs.h" 2 3 4
# 487 "/usr/include/features.h" 2 3 4
# 510 "/usr/include/features.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/gnu/stubs.h" 1 3 4
# 10 "/usr/include/x86_64-linux-gnu/gnu/stubs.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/gnu/stubs-64.h" 1 3 4
# 11 "/usr/include/x86_64-linux-gnu/gnu/stubs.h" 2 3 4
# 511 "/usr/include/features.h" 2 3 4
# 34 "/usr/include/x86_64-linux-gnu/bits/libc-header-start.h" 2 3 4
# 28 "/usr/include/stdio.h" 2 3 4





# 1 "/usr/lib/gcc/x86_64-linux-gnu/11/include/stddef.h" 1 3 4
# 209 "/usr/lib/gcc/x86_64-linux-gnu/11/include/stddef.h" 3 4

# 209 "/usr/lib/gcc/x86_64-linux-gnu/11/include/stddef.h" 3 4
typedef long unsigned int size_t;
# 34 "/usr/include/stdio.h" 2 3 4


# 1 "/usr/lib/gcc/x86_64-linux-gnu/11/include/stdarg.h" 1 3 4
# 40 "/usr/lib/gcc/x86_64-linux-gnu/11/include/stdarg.h" 3 4
typedef __builtin_va_list __gnuc_va_list;
# 37 "/usr/include/stdio.h" 2 3 4

# 1 "/usr/include/x86_64-linux-gnu/bits/types.h" 1 3 4
# 27 "/usr/include/x86_64-linux-gnu/bits/types.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wordsize.h" 1 3 4
# 28 "/usr/include/x86_64-linux-gnu/bits/types.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 1 3 4
# 19 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wordsize.h" 1 3 4
# 20 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 2 3 4
# 29 "/usr/include/x86_64-linux-gnu/bits/types.h" 2 3 4


typedef unsigned char __u_char;
typedef unsigned short int __u_short;
typedef unsigned int __u_int;
typedef unsigned long int __u_long;


typedef signed char __int8_t;
typedef unsigned char __uint8_t;
typedef signed short int __int16_t;
typedef unsigned short int __uint16_t;
typedef signed int __int32_t;
typedef unsigned int __uint32_t;

typedef signed long int __int64_t;
typedef unsigned long int __uint64_t;






typedef __int8_t __int_least8_t;
typedef __uint8_t __uint_least8_t;
typedef __int16_t __int_least16_t;
typedef __uint16_t __uint_least16_t;
typedef __int32_t __int_least32_t;
typedef __uint32_t __uint_least32_t;
typedef __int64_t __int_least64_t;
typedef __uint64_t __uint_least64_t;



typedef long int __quad_t;
typedef unsigned long int __u_quad_t;







typedef long int __intmax_t;
typedef unsigned long int __uintmax_t;
# 141 "/usr/include/x86_64-linux-gnu/bits/types.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/typesizes.h" 1 3 4
# 142 "/usr/include/x86_64-linux-gnu/bits/types.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/time64.h" 1 3 4
# 143 "/usr/include/x86_64-linux-gnu/bits/types.h" 2 3 4


typedef unsigned long int __dev_t;
typedef unsigned int __uid_t;
typedef unsigned int __gid_t;
typedef unsigned long int __ino_t;
typedef unsigned long int __ino64_t;
typedef unsigned int __mode_t;
typedef unsigned long int __nlink_t;
typedef long int __off_t;
typedef long int __off64_t;
typedef int __pid_t;
typedef struct { int __val[2]; } __fsid_t;
typedef long int __clock_t;
typedef unsigned long int __rlim_t;
typedef unsigned long int __rlim64_t;
typedef unsigned int __id_t;
typedef long int __time_t;
typedef unsigned int __useconds_t;
typedef long int __suseconds_t;
typedef long int __suseconds64_t;

typedef int __daddr_t;
typedef int __key_t;


typedef int __clockid_t;


typedef void * __timer_t;


typedef long int __blksize_t;




typedef long int __blkcnt_t;
typedef long int __blkcnt64_t;


typedef unsigned long int __fsblkcnt_t;
typedef unsigned long int __fsblkcnt64_t;


typedef unsigned long int __fsfilcnt_t;
typedef unsigned long int __fsfilcnt64_t;


typedef long int __fsword_t;

typedef long int __ssize_t;


typedef long int __syscall_slong_t;

typedef unsigned long int __syscall_ulong_t;



typedef __off64_t __loff_t;
typedef char *__caddr_t;


typedef long int __intptr_t;


typedef unsigned int __socklen_t;




typedef int __sig_atomic_t;
# 39 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/types/__fpos_t.h" 1 3 4




# 1 "/usr/include/x86_64-linux-gnu/bits/types/__mbstate_t.h" 1 3 4
# 13 "/usr/include/x86_64-linux-gnu/bits/types/__mbstate_t.h" 3 4
typedef struct
{
  int __count;
  union
  {
    unsigned int __wch;
    char __wchb[4];
  } __value;
} __mbstate_t;
# 6 "/usr/include/x86_64-linux-gnu/bits/types/__fpos_t.h" 2 3 4




typedef struct _G_fpos_t
{
  __off_t __pos;
  __mbstate_t __state;
} __fpos_t;
# 40 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/types/__fpos64_t.h" 1 3 4
# 10 "/usr/include/x86_64-linux-gnu/bits/types/__fpos64_t.h" 3 4
typedef struct _G_fpos64_t
{
  __off64_t __pos;
  __mbstate_t __state;
} __fpos64_t;
# 41 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/types/__FILE.h" 1 3 4



struct _IO_FILE;
typedef struct _IO_FILE __FILE;
# 42 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/types/FILE.h" 1 3 4



struct _IO_FILE;


typedef struct _IO_FILE FILE;
# 43 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/types/struct_FILE.h" 1 3 4
# 35 "/usr/include/x86_64-linux-gnu/bits/types/struct_FILE.h" 3 4
struct _IO_FILE;
struct _IO_marker;
struct _IO_codecvt;
struct _IO_wide_data;




typedef void _IO_lock_t;





struct _IO_FILE
{
  int _flags;


  char *_IO_read_ptr;
  char *_IO_read_end;
  char *_IO_read_base;
  char *_IO_write_base;
  char *_IO_write_ptr;
  char *_IO_write_end;
  char *_IO_buf_base;
  char *_IO_buf_end;


  char *_IO_save_base;
  char *_IO_backup_base;
  char *_IO_save_end;

  struct _IO_marker *_markers;

  struct _IO_FILE *_chain;

  int _fileno;
  int _flags2;
  __off_t _old_offset;


  unsigned short _cur_column;
  signed char _vtable_offset;
  char _shortbuf[1];

  _IO_lock_t *_lock;







  __off64_t _offset;

  struct _IO_codecvt *_codecvt;
  struct _IO_wide_data *_wide_data;
  struct _IO_FILE *_freeres_list;
  void *_freeres_buf;
  size_t __pad5;
  int _mode;

  char _unused2[15 * sizeof (int) - 4 * sizeof (void *) - sizeof (size_t)];
};
# 44 "/usr/include/stdio.h" 2 3 4
# 52 "/usr/include/stdio.h" 3 4
typedef __gnuc_va_list va_list;
# 63 "/usr/include/stdio.h" 3 4
typedef __off_t off_t;
# 77 "/usr/include/stdio.h" 3 4
typedef __ssize_t ssize_t;






typedef __fpos_t fpos_t;
# 133 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/stdio_lim.h" 1 3 4
# 134 "/usr/include/stdio.h" 2 3 4
# 143 "/usr/include/stdio.h" 3 4
extern FILE *stdin;
extern FILE *stdout;
extern FILE *stderr;






extern int remove (const char *__filename) __attribute__ ((__nothrow__ , __leaf__));

extern int rename (const char *__old, const char *__new) __attribute__ ((__nothrow__ , __leaf__));



extern int renameat (int __oldfd, const char *__old, int __newfd,
       const char *__new) __attribute__ ((__nothrow__ , __leaf__));
# 178 "/usr/include/stdio.h" 3 4
extern int fclose (FILE *__stream);
# 188 "/usr/include/stdio.h" 3 4
extern FILE *tmpfile (void)
  __attribute__ ((__malloc__)) __attribute__ ((__malloc__ (fclose, 1))) ;
# 205 "/usr/include/stdio.h" 3 4
extern char *tmpnam (char[20]) __attribute__ ((__nothrow__ , __leaf__)) ;




extern char *tmpnam_r (char __s[20]) __attribute__ ((__nothrow__ , __leaf__)) ;
# 222 "/usr/include/stdio.h" 3 4
extern char *tempnam (const char *__dir, const char *__pfx)
   __attribute__ ((__nothrow__ , __leaf__)) __attribute__ ((__malloc__)) __attribute__ ((__malloc__ (__builtin_free, 1)));






extern int fflush (FILE *__stream);
# 239 "/usr/include/stdio.h" 3 4
extern int fflush_unlocked (FILE *__stream);
# 258 "/usr/include/stdio.h" 3 4
extern FILE *fopen (const char *__restrict __filename,
      const char *__restrict __modes)
  __attribute__ ((__malloc__)) __attribute__ ((__malloc__ (fclose, 1))) ;




extern FILE *freopen (const char *__restrict __filename,
        const char *__restrict __modes,
        FILE *__restrict __stream) ;
# 293 "/usr/include/stdio.h" 3 4
extern FILE *fdopen (int __fd, const char *__modes) __attribute__ ((__nothrow__ , __leaf__))
  __attribute__ ((__malloc__)) __attribute__ ((__malloc__ (fclose, 1))) ;
# 308 "/usr/include/stdio.h" 3 4
extern FILE *fmemopen (void *__s, size_t __len, const char *__modes)
  __attribute__ ((__nothrow__ , __leaf__)) __attribute__ ((__malloc__)) __attribute__ ((__malloc__ (fclose, 1))) ;




extern FILE *open_memstream (char **__bufloc, size_t *__sizeloc) __attribute__ ((__nothrow__ , __leaf__))
  __attribute__ ((__malloc__)) __attribute__ ((__malloc__ (fclose, 1))) ;
# 328 "/usr/include/stdio.h" 3 4
extern void setbuf (FILE *__restrict __stream, char *__restrict __buf) __attribute__ ((__nothrow__ , __leaf__));



extern int setvbuf (FILE *__restrict __stream, char *__restrict __buf,
      int __modes, size_t __n) __attribute__ ((__nothrow__ , __leaf__));




extern void setbuffer (FILE *__restrict __stream, char *__restrict __buf,
         size_t __size) __attribute__ ((__nothrow__ , __leaf__));


extern void setlinebuf (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__));







extern int fprintf (FILE *__restrict __stream,
      const char *__restrict __format, ...);




extern int printf (const char *__restrict __format, ...);

extern int sprintf (char *__restrict __s,
      const char *__restrict __format, ...) __attribute__ ((__nothrow__));





extern int vfprintf (FILE *__restrict __s, const char *__restrict __format,
       __gnuc_va_list __arg);




extern int vprintf (const char *__restrict __format, __gnuc_va_list __arg);

extern int vsprintf (char *__restrict __s, const char *__restrict __format,
       __gnuc_va_list __arg) __attribute__ ((__nothrow__));



extern int snprintf (char *__restrict __s, size_t __maxlen,
       const char *__restrict __format, ...)
     __attribute__ ((__nothrow__)) __attribute__ ((__format__ (__printf__, 3, 4)));

extern int vsnprintf (char *__restrict __s, size_t __maxlen,
        const char *__restrict __format, __gnuc_va_list __arg)
     __attribute__ ((__nothrow__)) __attribute__ ((__format__ (__printf__, 3, 0)));
# 403 "/usr/include/stdio.h" 3 4
extern int vdprintf (int __fd, const char *__restrict __fmt,
       __gnuc_va_list __arg)
     __attribute__ ((__format__ (__printf__, 2, 0)));
extern int dprintf (int __fd, const char *__restrict __fmt, ...)
     __attribute__ ((__format__ (__printf__, 2, 3)));







extern int fscanf (FILE *__restrict __stream,
     const char *__restrict __format, ...) ;




extern int scanf (const char *__restrict __format, ...) ;

extern int sscanf (const char *__restrict __s,
     const char *__restrict __format, ...) __attribute__ ((__nothrow__ , __leaf__));





# 1 "/usr/include/x86_64-linux-gnu/bits/floatn.h" 1 3 4
# 119 "/usr/include/x86_64-linux-gnu/bits/floatn.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/floatn-common.h" 1 3 4
# 24 "/usr/include/x86_64-linux-gnu/bits/floatn-common.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/long-double.h" 1 3 4
# 25 "/usr/include/x86_64-linux-gnu/bits/floatn-common.h" 2 3 4
# 120 "/usr/include/x86_64-linux-gnu/bits/floatn.h" 2 3 4
# 431 "/usr/include/stdio.h" 2 3 4



extern int fscanf (FILE *__restrict __stream, const char *__restrict __format, ...) __asm__ ("" "__isoc99_fscanf")

                               ;
extern int scanf (const char *__restrict __format, ...) __asm__ ("" "__isoc99_scanf")
                              ;
extern int sscanf (const char *__restrict __s, const char *__restrict __format, ...) __asm__ ("" "__isoc99_sscanf") __attribute__ ((__nothrow__ , __leaf__))

                      ;
# 459 "/usr/include/stdio.h" 3 4
extern int vfscanf (FILE *__restrict __s, const char *__restrict __format,
      __gnuc_va_list __arg)
     __attribute__ ((__format__ (__scanf__, 2, 0))) ;





extern int vscanf (const char *__restrict __format, __gnuc_va_list __arg)
     __attribute__ ((__format__ (__scanf__, 1, 0))) ;


extern int vsscanf (const char *__restrict __s,
      const char *__restrict __format, __gnuc_va_list __arg)
     __attribute__ ((__nothrow__ , __leaf__)) __attribute__ ((__format__ (__scanf__, 2, 0)));





extern int vfscanf (FILE *__restrict __s, const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc99_vfscanf")



     __attribute__ ((__format__ (__scanf__, 2, 0))) ;
extern int vscanf (const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc99_vscanf")

     __attribute__ ((__format__ (__scanf__, 1, 0))) ;
extern int vsscanf (const char *__restrict __s, const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc99_vsscanf") __attribute__ ((__nothrow__ , __leaf__))



     __attribute__ ((__format__ (__scanf__, 2, 0)));
# 513 "/usr/include/stdio.h" 3 4
extern int fgetc (FILE *__stream);
extern int getc (FILE *__stream);





extern int getchar (void);






extern int getc_unlocked (FILE *__stream);
extern int getchar_unlocked (void);
# 538 "/usr/include/stdio.h" 3 4
extern int fgetc_unlocked (FILE *__stream);
# 549 "/usr/include/stdio.h" 3 4
extern int fputc (int __c, FILE *__stream);
extern int putc (int __c, FILE *__stream);





extern int putchar (int __c);
# 565 "/usr/include/stdio.h" 3 4
extern int fputc_unlocked (int __c, FILE *__stream);







extern int putc_unlocked (int __c, FILE *__stream);
extern int putchar_unlocked (int __c);






extern int getw (FILE *__stream);


extern int putw (int __w, FILE *__stream);







extern char *fgets (char *__restrict __s, int __n, FILE *__restrict __stream)
     __attribute__ ((__access__ (__write_only__, 1, 2)));
# 632 "/usr/include/stdio.h" 3 4
extern __ssize_t __getdelim (char **__restrict __lineptr,
                             size_t *__restrict __n, int __delimiter,
                             FILE *__restrict __stream) ;
extern __ssize_t getdelim (char **__restrict __lineptr,
                           size_t *__restrict __n, int __delimiter,
                           FILE *__restrict __stream) ;







extern __ssize_t getline (char **__restrict __lineptr,
                          size_t *__restrict __n,
                          FILE *__restrict __stream) ;







extern int fputs (const char *__restrict __s, FILE *__restrict __stream);





extern int puts (const char *__s);






extern int ungetc (int __c, FILE *__stream);






extern size_t fread (void *__restrict __ptr, size_t __size,
       size_t __n, FILE *__restrict __stream) ;




extern size_t fwrite (const void *__restrict __ptr, size_t __size,
        size_t __n, FILE *__restrict __s);
# 702 "/usr/include/stdio.h" 3 4
extern size_t fread_unlocked (void *__restrict __ptr, size_t __size,
         size_t __n, FILE *__restrict __stream) ;
extern size_t fwrite_unlocked (const void *__restrict __ptr, size_t __size,
          size_t __n, FILE *__restrict __stream);







extern int fseek (FILE *__stream, long int __off, int __whence);




extern long int ftell (FILE *__stream) ;




extern void rewind (FILE *__stream);
# 736 "/usr/include/stdio.h" 3 4
extern int fseeko (FILE *__stream, __off_t __off, int __whence);




extern __off_t ftello (FILE *__stream) ;
# 760 "/usr/include/stdio.h" 3 4
extern int fgetpos (FILE *__restrict __stream, fpos_t *__restrict __pos);




extern int fsetpos (FILE *__stream, const fpos_t *__pos);
# 786 "/usr/include/stdio.h" 3 4
extern void clearerr (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__));

extern int feof (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__)) ;

extern int ferror (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__)) ;



extern void clearerr_unlocked (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__));
extern int feof_unlocked (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__)) ;
extern int ferror_unlocked (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__)) ;







extern void perror (const char *__s);




extern int fileno (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__)) ;




extern int fileno_unlocked (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__)) ;
# 823 "/usr/include/stdio.h" 3 4
extern int pclose (FILE *__stream);





extern FILE *popen (const char *__command, const char *__modes)
  __attribute__ ((__malloc__)) __attribute__ ((__malloc__ (pclose, 1))) ;






extern char *ctermid (char *__s) __attribute__ ((__nothrow__ , __leaf__))
  __attribute__ ((__access__ (__write_only__, 1)));
# 867 "/usr/include/stdio.h" 3 4
extern void flockfile (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__));



extern int ftrylockfile (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__)) ;


extern void funlockfile (FILE *__stream) __attribute__ ((__nothrow__ , __leaf__));
# 885 "/usr/include/stdio.h" 3 4
extern int __uflow (FILE *);
extern int __overflow (FILE *, int);
# 902 "/usr/include/stdio.h" 3 4

# 2 "data/user_code.c" 2


# 3 "data/user_code.c"
int add(int a, int b) {
    int result = a + b;
    return result;
}

int main() {
    int x = 5;
    int y = 10;
    int sum = add(x, y);
    printf("Sum: %d\n", sum);
    return 0;
}
// example1.c

int main() {
    int a = 10;
    float b = 20.5;
    if (a < b) {
        return a;
    } else {
        return b;
    }
}
// example2.c

int add(int a, int b) {
    return a + b;
}

int main() {
    int x = 5;
    int y = 10;
    float z = add(x, y) * 2.5;
    if (z > 20.0) {
        return 1;
    } else {
        return 0;
    }
}
#include <stdio.h>

int add(int a, int b) {
    int result = a + b;
    return result;
}

int main() {
    int x = 5;
    int y = 10;
    int sum = add(x, y);
    printf("Sum: %d\n", sum);
    return 0;
}#!/bin/bash

# Create top-level files
touch README.md LICENSE requirements.txt setup.py .gitignore

# Create directories and subdirectories
mkdir -p docs src/config src/lexer src/parser src/analyzer src/utils src/api tests/test_lexer tests/test_parser tests/test_analyzer tests/test_utils scripts data/sample_c_codes

# Create files in docs
touch docs/architecture.md docs/user_guide.md docs/api_reference.md

# Create files in src
touch src/__init__.py src/main.py
touch src/config/__init__.py src/config/settings.py
touch src/lexer/__init__.py src/lexer/tokenizer.py src/lexer/tokens.py
touch src/parser/__init__.py src/parser/ast_nodes.py src/parser/parser.py src/parser/grammar.py
touch src/analyzer/__init__.py src/analyzer/semantic_analyzer.py src/analyzer/symbol_table.py
touch src/utils/__init__.py src/utils/logger.py src/utils/helpers.py
touch src/api/__init__.py src/api/endpoints.py src/api/schemas.py

# Create files in tests
touch tests/__init__.py
touch tests/test_lexer/__init__.py tests/test_lexer/test_tokenizer.py
touch tests/test_parser/__init__.py tests/test_parser/test_parser.py
touch tests/test_analyzer/__init__.py tests/test_analyzer/test_semantic_analyzer.py
touch tests/test_utils/__init__.py tests/test_utils/test_helpers.py

# Create files in scripts
touch scripts/build.sh scripts/run_tests.sh

# Create sample C code files in data/sample_c_codes
touch data/sample_c_codes/example1.c data/sample_c_codes/example2.c

echo "File structure created successfully."
# src/lexer/error_handler.py

import json
import os

class CompilerError:
    """Represents a single error in the compiler."""
    def __init__(self, error_type, message, lineno, column):
        self.error_type = error_type  # 'Lexical', 'Syntax', 'Semantic', etc.
        self.message = message
        self.lineno = lineno
        self.column = column

    def to_dict(self):
        return {
            "type": self.error_type,
            "message": self.message,
            "line": self.lineno,
            "column": self.column
        }

class ErrorHandler:
    """Centralized error handling for the compiler."""
    def __init__(self):
        self.errors = []

    def add_error(self, error_type, message, lineno, column):
        """Record a new compiler error."""
        err = CompilerError(error_type, message, lineno, column)
        self.errors.append(err)
        # Print a concise message for immediate feedback
        print(f"{error_type} Error at line {lineno}, column {column}: {message}")

    def has_errors(self):
        return len(self.errors) > 0

    def display_errors(self):
        """Display all errors on the console."""
        if not self.errors:
            print("No errors found.")
            return

        print("\n--- Compilation Errors ---")
        for e in self.errors:
            print(f"{e.error_type} Error at line {e.lineno}, column {e.column}: {e.message}")
        print("--------------------------\n")

    def export_to_json(self, file_path='data/errors.json'):
        """Export errors to a JSON file for easier reference."""
        directory = os.path.dirname(file_path)
        if directory and not os.path.exists(directory):
            os.makedirs(directory)

        with open(file_path, 'w') as jf:
            json.dump([err.to_dict() for err in self.errors], jf, indent=4)
        print(f"Errors exported to {file_path}")

# Initialize the error handler
error_handler = ErrorHandler()
# # import json
# # import os

# # class Symbol:
# #     """Represents an identifier in the symbol table."""
# #     def __init__(self, name, token_type, lineno):
# #         self.name = name
# #         self.token_type = token_type
# #         self.lineno = lineno

# #     def __repr__(self):
# #         return f"Symbol(name='{self.name}', type='{self.token_type}', line={self.lineno})"

# #     def to_dict(self):
# #         """Converts the Symbol instance to a dictionary for JSON serialization."""
# #         return {
# #             "name": self.name,
# #             "type": self.token_type,
# #             "line": self.lineno
# #         }

# # class SymbolTable:
# #     """Stores symbols collected during lexical analysis."""
# #     def __init__(self):
# #         self.symbols = {}

# #     def add_symbol(self, name, token_type, lineno):
# #         if name not in self.symbols:
# #             self.symbols[name] = Symbol(name, token_type, lineno)
# #             print(f"Added to Symbol Table: {self.symbols[name]}")
# #         else:
# #             print(f"Symbol '{name}' already exists in the Symbol Table.")

# #     def display(self):
# #         print("\n--- Symbol Table ---")
# #         if not self.symbols:
# #             print("No symbols collected.")
# #         else:
# #             for symbol in self.symbols.values():
# #                 print(f"{symbol}")
# #         print("---------------------\n")

# #     def export_to_json(self, file_path):
# #         """Exports the symbol table to a JSON file."""
# #         # Create the directory if it does not exist
# #         directory = os.path.dirname(file_path)
# #         if not os.path.exists(directory):
# #             os.makedirs(directory)
        
# #         with open(file_path, 'w') as json_file:
# #             json.dump(
# #                 {name: symbol.to_dict() for name, symbol in self.symbols.items()},
# #                 json_file,
# #                 indent=4
# #             )
# #         print(f"Symbol table exported to {file_path}")



# # src/lexer/symbol_table.py

# import json
# import os

# class Symbol:
#     """Represents an identifier in the symbol table."""
#     def __init__(self, name, token_type, lineno, column):
#         self.name = name
#         self.token_type = token_type
#         self.lineno = lineno
#         self.column = column

#     def __repr__(self):
#         return f"Symbol(name='{self.name}', type='{self.token_type}', line={self.lineno}, column={self.column})"

#     def to_dict(self):
#         """Converts the Symbol instance to a dictionary for JSON serialization."""
#         return {
#             "name": self.name,
#             "type": self.token_type,
#             "line": self.lineno,
#             "column": self.column
#         }

# class SymbolTable:
#     """Stores symbols collected during lexical analysis."""
#     def __init__(self):
#         self.symbols = {}

#     def add_symbol(self, name, token_type, lineno, column):
#         if name not in self.symbols:
#             self.symbols[name] = Symbol(name, token_type, lineno, column)
#             print(f"Added to Symbol Table: {self.symbols[name]}")
#         else:
#             print(f"Symbol '{name}' already exists in the Symbol Table.")

#     def display(self):
#         print("\n--- Symbol Table ---")
#         if not self.symbols:
#             print("No symbols collected.")
#         else:
#             for symbol in self.symbols.values():
#                 print(f"{symbol}")
#         print("---------------------\n")

#     def export_to_json(self, file_path):
#         """Exports the symbol table to a JSON file."""
#         # Create the directory if it does not exist
#         directory = os.path.dirname(file_path)
#         if not os.path.exists(directory):
#             os.makedirs(directory)
        
#         with open(file_path, 'w') as json_file:
#             json.dump(
#                 {name: symbol.to_dict() for name, symbol in self.symbols.items()},
#                 json_file,
#                 indent=4
#             )
#         print(f"Symbol table exported to {file_path}")




# src/lexer/symbol_table.py

import json
import os

class Symbol:
    """Represents an identifier in the symbol table."""
    def __init__(self, name, token_type, lineno, column):
        self.name = name
        self.token_type = token_type
        self.lineno = lineno
        self.column = column

    def __repr__(self):
        return f"Symbol(name='{self.name}', type='{self.token_type}', line={self.lineno}, column={self.column})"

    def to_dict(self):
        """Converts the Symbol instance to a dictionary for JSON serialization."""
        return {
            "name": self.name,
            "type": self.token_type,
            "line": self.lineno,
            "column": self.column
        }

class SymbolTable:
    """Stores symbols collected during lexical analysis."""
    def __init__(self):
        self.symbols = {}

    def add_symbol(self, name, token_type, lineno, column):
        if name not in self.symbols:
            self.symbols[name] = Symbol(name, token_type, lineno, column)
            print(f"Added to Symbol Table: {self.symbols[name]}")
        else:
            print(f"Symbol '{name}' already exists in the Symbol Table.")

    def display(self):
        print("\n--- Symbol Table ---")
        if not self.symbols:
            print("No symbols collected.")
        else:
            for symbol in self.symbols.values():
                print(f"{symbol}")
        print("---------------------\n")

    def export_to_json(self, file_path):
        """Exports the symbol table to a JSON file."""
        # Create the directory if it does not exist
        directory = os.path.dirname(file_path)
        if not os.path.exists(directory):
            os.makedirs(directory)
        
        with open(file_path, 'w') as json_file:
            json.dump(
                {name: symbol.to_dict() for name, symbol in self.symbols.items()},
                json_file,
                indent=4
            )
        print(f"Symbol table exported to {file_path}")
# src/lexer/tokenizer.py

import ply.lex as lex
from .symbol_table import SymbolTable
from .tokens import tokens, reserved  # Import both tokens and reserved
from .error_handler import error_handler

# Initialize the symbol table
symbol_table = SymbolTable()

# Regular expression rules for simple tokens
t_ASSIGN    = r'='
t_SEMICOLON = r';'
t_LPAREN    = r'\('
t_RPAREN    = r'\)'
t_LBRACE    = r'\{'
t_RBRACE    = r'\}'
t_PLUS      = r'\+'
t_MINUS     = r'-'
t_MULTIPLY  = r'\*'
t_DIVIDE    = r'/'
t_LT        = r'<'
t_GT        = r'>'
t_LE        = r'<='
t_GE        = r'>='
t_EQ        = r'=='
t_NE        = r'!='
t_COMMA     = r','

# Reserved words
reserved = {
    'int': 'INT',
    'float': 'FLOAT',
    'if': 'IF',
    'else': 'ELSE',
    'return': 'RETURN',
}

# Define token actions
def t_IDENTIFIER(t):
    r'[A-Za-z_][A-Za-z0-9_]*'
    t.type = reserved.get(t.value, 'IDENTIFIER')  # Check for reserved words
    t.column = find_column(t.lexer.lexdata, t)  # Add column attribute
    symbol_table.add_symbol(t.value, t.type, t.lineno, t.column)
    return t

def t_FLOAT_LITERAL(t):
    r'\d+\.\d+'
    t.value = float(t.value)
    t.column = find_column(t.lexer.lexdata, t)
    return t

def t_INT_LITERAL(t):
    r'\d+'
    t.value = int(t.value)
    t.column = find_column(t.lexer.lexdata, t)
    return t

# Track newlines
def t_newline(t):
    r'\n+'
    t.lexer.lineno += len(t.value)

# Ignored characters
t_ignore = ' \t'

# Comments
def t_comment_singleline(t):
    r'//.*'
    pass

def t_comment_multiline(t):
    r'/\*(.|\n)*?\*/'
    t.lexer.lineno += t.value.count('\n')
    pass

# Error handling rule
def t_error(t):
    column = find_column(t.lexer.lexdata, t)
    error_handler.add_error('Lexical', f"Illegal character '{t.value[0]}'", t.lineno, column)
    t.lexer.skip(1)

# Compute column numbers
def find_column(input, token):
    """
    Compute the column number of a token.
    """
    last_cr = input.rfind('\n', 0, token.lexpos)
    if last_cr < 0:
        last_cr = -1
    return (token.lexpos - last_cr)

# Build the lexer
lexer = lex.lex()

def tokenize(data):
    """
    Tokenize the input data and return a list of tokens with column information.
    """
    lexer.input(data)
    tokens_list = []
    while True:
        tok = lexer.token()
        if not tok:
            break
        tok.column = find_column(data, tok)  # Assign column number
        tokens_list.append(tok)
    return tokens_list
# src/lexer/tokens.py

# List of token names. This is always required by PLY
tokens = [
    # Operators
    'PLUS',
    'MINUS',
    'MULTIPLY',
    'DIVIDE',
    'ASSIGN',

    # Delimiters
    'LPAREN',
    'RPAREN',
    'LBRACE',
    'RBRACE',
    'SEMICOLON',
    'COMMA',

    # Identifiers and literals
    'IDENTIFIER',
    'INT_LITERAL',
    'FLOAT_LITERAL',
    
    # Comparison Operators
    'LT',    # <
    'GT',    # >
    'LE',    # <=
    'GE',    # >=
    'EQ',    # ==
    'NE',    # !=
]

# Reserved words mapping
reserved = {
    'int': 'INT',
    'float': 'FLOAT',
    'if': 'IF',
    'else': 'ELSE',
    # 'while': 'WHILE',
    'return': 'RETURN',
}

# Combine tokens and reserved words
tokens += list(reserved.values())
# src/lexer/__init__.py

from .tokenizer import tokenize, symbol_table

__all__ = ['tokenize', 'symbol_table']
# # src/main.py

# import sys
# from lexer import tokenize, symbol_table  # Import the lexer and symbol table
# from parser import parse_tokens  # Import the parser
# from utils.ast_visualizer import ASTVisualizer
# import os

# file_path_for_symbol_table = os.path.join(os.getcwd(), "output/symbol_table.json")

# def main():
#     if len(sys.argv) != 2:
#         print("Usage: python main.py <path_to_c_file>")
#         sys.exit(1)

#     file_path = sys.argv[1]

#     try:
#         with open(file_path, 'r') as file:
#             data = file.read()
#     except FileNotFoundError:
#         print(f"File '{file_path}' not found.")
#         sys.exit(1)

#     print("\n--- Lexical Analysis ---")
#     tokens_list = tokenize(data)

#     print("\nTokens:")
#     for tok in tokens_list:
#         print(f"Type: {tok.type}, Value: {tok.value}, Line: {tok.lineno}, Column: {tok.column}")

#     print("\n--- Syntax Analysis ---")
#     ast = parse_tokens(tokens_list)

#     print("\nAbstract Syntax Tree (AST):")
#     print(ast)

#     # Visualize the AST
#     print("\n--- AST Visualization ---")
#     visualizer = ASTVisualizer()
#     visualizer.visualize(ast, output_filename='ast_output')  # You can change the filename as needed

#     # Display the Symbol Table
#     symbol_table.display()

#     symbol_table.export_to_json(file_path_for_symbol_table)

# if __name__ == '__main__':
#     main()

# src/main.py

import sys
from lexer import tokenize, symbol_table  # Import the lexer and symbol table
from parser import parse_tokens  # Import the parser
from utils.ast_visualizer import ASTVisualizer
from lexer.error_handler import error_handler  # Import the error handler
import os
import json

file_path_for_symbol_table = os.path.join(os.getcwd(), "output/symbol_table.json")

def main():
    if len(sys.argv) != 2:
        print("Usage: python main.py <path_to_code_file>")
        sys.exit(1)

    file_path = sys.argv[1]

    try:
        with open(file_path, 'r') as file:
            data = file.read()
    except FileNotFoundError:
        print(f"File '{file_path}' not found.")
        sys.exit(1)

    print("\n--- Lexical Analysis ---")
    tokens_list = tokenize(data)

    print("\nTokens:")
    for tok in tokens_list:
        print(f"Type: {tok.type}, Value: {tok.value}, Line: {tok.lineno}, Column: {tok.column}")

    print("\n--- Syntax Analysis ---")
    ast = parse_tokens(tokens_list)

    print("\nAbstract Syntax Tree (AST):")
    print(ast)

    if not error_handler.has_errors() and ast is not None:
        # Visualize the AST
        print("\n--- AST Visualization ---")
        visualizer = ASTVisualizer()
        visualizer.visualize(ast, output_filename='ast_output')  # Saves as ast_output.png
    else:
        print("\nAST not generated due to syntax errors.")

    # Display and export errors
    error_handler.display_errors()
    error_handler.export_to_json('data/errors.json')

    # Display the Symbol Table
    symbol_table.display()

    symbol_table.export_to_json(file_path_for_symbol_table)

if __name__ == '__main__':
    main()
# src/parser/ast_nodes.py

import itertools

class ASTNode:
    _id_iter = itertools.count()

    def __init__(self):
        self.id = next(ASTNode._id_iter)

    def get_label(self):
        return self.__class__.__name__

    def __repr__(self):
        return f"{self.__class__.__name__}()"

class Program(ASTNode):
    def __init__(self, declarations):
        super().__init__()
        self.declarations = declarations

    def get_label(self):
        return "Program"

    def __repr__(self):
        return f"Program(declarations={self.declarations})"

class FunctionDeclaration(ASTNode):
    def __init__(self, return_type, name, params, body):
        super().__init__()
        self.return_type = return_type
        self.name = name
        self.params = params
        self.body = body

    def get_label(self):
        return f"FunctionDeclaration\n{self.return_type} {self.name}"

    def __repr__(self):
        return f"FunctionDeclaration(return_type='{self.return_type}', name='{self.name}', params={self.params}, body={self.body})"

class VariableDeclaration(ASTNode):
    def __init__(self, var_type, name, initializer=None):
        super().__init__()
        self.var_type = var_type
        self.name = name
        self.initializer = initializer

    def get_label(self):
        if self.initializer:
            return f"VariableDeclaration\n{self.var_type} {self.name} = ..."
        return f"VariableDeclaration\n{self.var_type} {self.name}"

    def __repr__(self):
        return f"VariableDeclaration(var_type='{self.var_type}', name='{self.name}', initializer={self.initializer})"

class IfStatement(ASTNode):
    def __init__(self, condition, then_branch, else_branch=None):
        super().__init__()
        self.condition = condition
        self.then_branch = then_branch
        self.else_branch = else_branch

    def get_label(self):
        return "IfStatement"

    def __repr__(self):
        return f"IfStatement(condition={self.condition}, then_branch={self.then_branch}, else_branch={self.else_branch})"

class ReturnStatement(ASTNode):
    def __init__(self, expression):
        super().__init__()
        self.expression = expression

    def get_label(self):
        return "ReturnStatement"

    def __repr__(self):
        return f"ReturnStatement(expression={self.expression})"

class BinaryOperation(ASTNode):
    def __init__(self, left, operator, right):
        super().__init__()
        self.left = left
        self.operator = operator
        self.right = right

    def get_label(self):
        return f"BinaryOperation\n'{self.operator}'"

    def __repr__(self):
        return f"BinaryOperation(left={self.left}, operator='{self.operator}', right={self.right})"

class UnaryOperation(ASTNode):
    def __init__(self, operator, operand):
        super().__init__()
        self.operator = operator
        self.operand = operand

    def get_label(self):
        return f"UnaryOperation\n'{self.operator}'"

    def __repr__(self):
        return f"UnaryOperation(operator='{self.operator}', operand={self.operand})"

class Identifier(ASTNode):
    def __init__(self, name):
        super().__init__()
        self.name = name

    def get_label(self):
        return f"Identifier\n{self.name}"

    def __repr__(self):
        return f"Identifier(name='{self.name}')"

class Literal(ASTNode):
    def __init__(self, value):
        super().__init__()
        self.value = value

    def get_label(self):
        return f"Literal\n{self.value}"

    def __repr__(self):
        return f"Literal(value={self.value})"

class CompoundStatement(ASTNode):
    def __init__(self, statements):
        super().__init__()
        self.statements = statements

    def get_label(self):
        return "CompoundStatement"

    def __repr__(self):
        return f"CompoundStatement(statements={self.statements})"

# class FunctionCall(ASTNode):
#     def __init__(self, name, args):
#         super().__init__()
#         self.name = name  # Should be an Identifier instance
#         self.args = args  # List of expressions

#     def get_label(self):
#         return f"FunctionCall\n{self.name.name}"

#     def __repr__(self):
#         return f"FunctionCall(name={self.name}, args={self.args})"

class FunctionCall(ASTNode):
    def __init__(self, name, args):
        super().__init__()
        self.name = name  # Should be an Identifier instance
        self.args = args  # List of expressions

    def get_label(self):
        return f"FunctionCall\n{self.name.name}"

    def __repr__(self):
        return f"FunctionCall(name={self.name}, args={self.args})"# src/parser/parser.py

import ply.yacc as yacc
from lexer.tokens import tokens  # Import tokens list from lexer/tokens.py

from lexer.error_handler import error_handler  # Import the error handler

from .ast_nodes import (
    Program,
    FunctionDeclaration,
    VariableDeclaration,
    IfStatement,
    ReturnStatement,
    BinaryOperation,
    UnaryOperation,
    FunctionCall,  # Added
    Identifier,
    Literal,
    CompoundStatement
)

# Define precedence rules to resolve ambiguities
precedence = (
    ('left', 'EQ', 'NE'),              # Equality operators
    ('left', 'LT', 'LE', 'GT', 'GE'),   # Relational operators
    ('left', 'PLUS', 'MINUS'),         # Addition and subtraction
    ('left', 'MULTIPLY', 'DIVIDE'),    # Multiplication and division
    ('right', 'UMINUS'),               # Unary minus
)

# Dictionary to hold all AST nodes (optional, can be used for tracking)
ast_nodes = []

def p_program(p):
    """program : declarations"""
    p[0] = Program(p[1])
    ast_nodes.append(p[0])

def p_declarations(p):
    """declarations : declarations declaration
                    | declaration"""
    if len(p) == 3:
        p[0] = p[1] + [p[2]]
    else:
        p[0] = [p[1]]

def p_declaration(p):
    """declaration : var_declaration
                   | func_declaration"""
    p[0] = p[1]

def p_var_declaration(p):
    """var_declaration : type_specifier IDENTIFIER SEMICOLON
                       | type_specifier IDENTIFIER ASSIGN expression SEMICOLON"""
    var_type = p[1]
    var_name = p[2]
    if len(p) == 4:
        initializer = None
    else:
        initializer = p[4]
    p[0] = VariableDeclaration(var_type=var_type, name=var_name, initializer=initializer)

def p_type_specifier(p):
    """type_specifier : INT
                      | FLOAT"""
    p[0] = p[1]

def p_func_declaration(p):
    """func_declaration : type_specifier IDENTIFIER LPAREN params RPAREN compound_stmt"""
    return_type = p[1]
    func_name = p[2]
    params = p[4]
    body = p[6]
    p[0] = FunctionDeclaration(return_type=return_type, name=func_name, params=params, body=body)

def p_params(p):
    """params : param_list
              | empty"""
    p[0] = p[1] if p[1] is not None else []

def p_param_list(p):
    """param_list : param_list COMMA param
                  | param"""
    if len(p) == 4:
        p[0] = p[1] + [p[3]]
    else:
        p[0] = [p[1]]

def p_param(p):
    """param : type_specifier IDENTIFIER"""
    var_type = p[1]
    var_name = p[2]
    return VariableDeclaration(var_type=var_type, name=var_name)

def p_compound_stmt(p):
    """compound_stmt : LBRACE stmt_list RBRACE"""
    p[0] = CompoundStatement(statements=p[2])

def p_stmt_list(p):
    """stmt_list : stmt_list statement
                 | empty"""
    if len(p) == 3:
        p[0] = p[1] + [p[2]]
    else:
        p[0] = []

def p_statement(p):
    """statement : expr_stmt
                 | var_declaration
                 | compound_stmt
                 | if_stmt
                 | return_stmt"""
    p[0] = p[1]

def p_if_stmt(p):
    """if_stmt : IF LPAREN expression RPAREN statement
               | IF LPAREN expression RPAREN statement ELSE statement"""
    condition = p[3]
    then_branch = p[5]
    else_branch = p[7] if len(p) > 6 else None
    p[0] = IfStatement(condition=condition, then_branch=then_branch, else_branch=else_branch)

def p_return_stmt(p):
    """return_stmt : RETURN expression SEMICOLON"""
    expr = p[2]
    p[0] = ReturnStatement(expression=expr)

def p_expr_stmt(p):
    """expr_stmt : expression SEMICOLON"""
    p[0] = p[1]

def p_expression_binop(p):
    """expression : expression PLUS expression
                  | expression MINUS expression
                  | expression MULTIPLY expression
                  | expression DIVIDE expression
                  | expression LT expression
                  | expression GT expression
                  | expression LE expression
                  | expression GE expression
                  | expression EQ expression
                  | expression NE expression"""
    left = p[1]
    operator = p[2]
    right = p[3]
    p[0] = BinaryOperation(left=left, operator=operator, right=right)

def p_expression_uminus(p):
    """expression : MINUS expression %prec UMINUS"""
    operand = p[2]
    p[0] = UnaryOperation(operator='-', operand=operand)

def p_expression_group(p):
    """expression : LPAREN expression RPAREN"""
    p[0] = p[2]

def p_expression_number(p):
    """expression : INT_LITERAL
                  | FLOAT_LITERAL"""
    p[0] = Literal(value=p[1])

def p_expression_identifier(p):
    """expression : IDENTIFIER"""
    p[0] = Identifier(name=p[1])

def p_expression_call(p):
    """expression : IDENTIFIER LPAREN arg_list RPAREN"""
    p[0] = FunctionCall(name=Identifier(p[1]), args=p[3])

def p_arg_list(p):
    """arg_list : arg_list COMMA expression
                | expression
                | empty"""
    if len(p) == 4:
        p[0] = p[1] + [p[3]]
    elif len(p) == 2:
        if p[1] is None:
            p[0] = []
        else:
            p[0] = [p[1]]
    else:
        p[0] = []

def p_empty(p):
    """empty :"""
    p[0] = None  # Explicitly set to None for clarity

# def p_error(p):
#     if p:
#         column = getattr(p, 'column', 'Unknown')  # Safely get 'column' attribute
#         print(f"Syntax error at token '{p.value}', line {p.lineno}, column {column}")
#     else:
#         print("Syntax error at EOF")

def p_error(p):
    if p:
        column = getattr(p, 'column', 'Unknown')  # Safely get 'column' attribute
        error_handler.add_error('Syntax', f"Syntax error at token '{p.value}'", p.lineno, column)
    else:
        error_handler.add_error('Syntax', "Syntax error at EOF", '?', '?')

# Build the parser
parser = yacc.yacc()

def parse_tokens(tokens_list):
    """
    Parse the list of tokens and return the AST.
    """
    from collections import deque

    token_queue = deque(tokens_list)

    class TokenLexer:
        def __init__(self, tokens):
            self.tokens = tokens

        def token(self):
            if self.tokens:
                return self.tokens.popleft()
            else:
                return None

    lexer = TokenLexer(token_queue)
    ast = parser.parse(lexer=lexer)
    return ast

# parsetab.py
# This file is automatically generated. Do not edit.
# pylint: disable=W,C,R
_tabversion = '3.10'

_lr_method = 'LALR'

_lr_signature = 'leftEQNEleftLTLEGTGEleftPLUSMINUSleftMULTIPLYDIVIDErightUMINUSASSIGN COMMA DIVIDE ELSE EQ FLOAT FLOAT_LITERAL GE GT IDENTIFIER IF INT INT_LITERAL LBRACE LE LPAREN LT MINUS MULTIPLY NE PLUS RBRACE RETURN RPAREN SEMICOLONprogram : declarationsdeclarations : declarations declaration\n                    | declarationdeclaration : var_declaration\n                   | func_declarationvar_declaration : type_specifier IDENTIFIER SEMICOLON\n                       | type_specifier IDENTIFIER ASSIGN expression SEMICOLONtype_specifier : INT\n                      | FLOATfunc_declaration : type_specifier IDENTIFIER LPAREN params RPAREN compound_stmtparams : param_list\n              | emptyparam_list : param_list COMMA param\n                  | paramparam : type_specifier IDENTIFIERcompound_stmt : LBRACE stmt_list RBRACEstmt_list : stmt_list statement\n                 | emptystatement : expr_stmt\n                 | var_declaration\n                 | compound_stmt\n                 | if_stmt\n                 | return_stmtif_stmt : IF LPAREN expression RPAREN statement\n               | IF LPAREN expression RPAREN statement ELSE statementreturn_stmt : RETURN expression SEMICOLONexpr_stmt : expression SEMICOLONexpression : expression PLUS expression\n                  | expression MINUS expression\n                  | expression MULTIPLY expression\n                  | expression DIVIDE expression\n                  | expression LT expression\n                  | expression GT expression\n                  | expression LE expression\n                  | expression GE expression\n                  | expression EQ expression\n                  | expression NE expressionexpression : MINUS expression %prec UMINUSexpression : LPAREN expression RPARENexpression : INT_LITERAL\n                  | FLOAT_LITERALexpression : IDENTIFIERexpression : IDENTIFIER LPAREN arg_list RPARENarg_list : arg_list COMMA expression\n                | expression\n                | emptyempty :'
    
_lr_action_items = {'INT':([0,2,3,4,5,9,11,13,26,41,56,57,61,62,64,65,66,67,68,69,70,75,80,81,82,83,84,],[7,7,-3,-4,-5,-2,-6,7,-7,7,-10,-47,7,-18,-16,-17,-19,-20,-21,-22,-23,-27,-26,7,-24,7,-25,]),'FLOAT':([0,2,3,4,5,9,11,13,26,41,56,57,61,62,64,65,66,67,68,69,70,75,80,81,82,83,84,],[8,8,-3,-4,-5,-2,-6,8,-7,8,-10,-47,8,-18,-16,-17,-19,-20,-21,-22,-23,-27,-26,8,-24,8,-25,]),'$end':([1,2,3,4,5,9,11,26,56,64,],[0,-1,-3,-4,-5,-2,-6,-7,-10,-16,]),'IDENTIFIER':([6,7,8,11,12,16,17,20,25,26,27,28,29,30,31,32,33,34,35,36,57,60,61,62,64,65,66,67,68,69,70,72,74,75,77,80,81,82,83,84,],[10,-8,-9,-6,14,14,14,39,14,-7,14,14,14,14,14,14,14,14,14,14,-47,14,14,-18,-16,-17,-19,-20,-21,-22,-23,76,14,-27,14,-26,14,-24,14,-25,]),'SEMICOLON':([10,14,15,18,19,37,45,46,47,48,49,50,51,52,53,54,55,59,71,76,78,],[11,-42,26,-40,-41,-38,-28,-29,-30,-31,-32,-33,-34,-35,-36,-37,-39,-43,75,11,80,]),'ASSIGN':([10,76,],[12,12,]),'LPAREN':([10,11,12,14,16,17,25,26,27,28,29,30,31,32,33,34,35,36,57,60,61,62,64,65,66,67,68,69,70,73,74,75,77,80,81,82,83,84,],[13,-6,17,25,17,17,17,-7,17,17,17,17,17,17,17,17,17,17,-47,17,17,-18,-16,-17,-19,-20,-21,-22,-23,77,17,-27,17,-26,17,-24,17,-25,]),'RBRACE':([11,26,57,61,62,64,65,66,67,68,69,70,75,80,82,84,],[-6,-7,-47,64,-18,-16,-17,-19,-20,-21,-22,-23,-27,-26,-24,-25,]),'LBRACE':([11,26,40,57,61,62,64,65,66,67,68,69,70,75,80,81,82,83,84,],[-6,-7,57,-47,57,-18,-16,-17,-19,-20,-21,-22,-23,-27,-26,57,-24,57,-25,]),'IF':([11,26,57,61,62,64,65,66,67,68,69,70,75,80,81,82,83,84,],[-6,-7,-47,73,-18,-16,-17,-19,-20,-21,-22,-23,-27,-26,73,-24,73,-25,]),'RETURN':([11,26,57,61,62,64,65,66,67,68,69,70,75,80,81,82,83,84,],[-6,-7,-47,74,-18,-16,-17,-19,-20,-21,-22,-23,-27,-26,74,-24,74,-25,]),'MINUS':([11,12,14,15,16,17,18,19,25,26,27,28,29,30,31,32,33,34,35,36,37,38,43,45,46,47,48,49,50,51,52,53,54,55,57,59,60,61,62,63,64,65,66,67,68,69,70,71,74,75,77,78,79,80,81,82,83,84,],[-6,16,-42,28,16,16,-40,-41,16,-7,16,16,16,16,16,16,16,16,16,16,-38,28,28,-28,-29,-30,-31,28,28,28,28,28,28,-39,-47,-43,16,16,-18,28,-16,-17,-19,-20,-21,-22,-23,28,16,-27,16,28,28,-26,16,-24,16,-25,]),'INT_LITERAL':([11,12,16,17,25,26,27,28,29,30,31,32,33,34,35,36,57,60,61,62,64,65,66,67,68,69,70,74,75,77,80,81,82,83,84,],[-6,18,18,18,18,-7,18,18,18,18,18,18,18,18,18,18,-47,18,18,-18,-16,-17,-19,-20,-21,-22,-23,18,-27,18,-26,18,-24,18,-25,]),'FLOAT_LITERAL':([11,12,16,17,25,26,27,28,29,30,31,32,33,34,35,36,57,60,61,62,64,65,66,67,68,69,70,74,75,77,80,81,82,83,84,],[-6,19,19,19,19,-7,19,19,19,19,19,19,19,19,19,19,-47,19,19,-18,-16,-17,-19,-20,-21,-22,-23,19,-27,19,-26,19,-24,19,-25,]),'ELSE':([11,26,64,66,67,68,69,70,75,80,82,84,],[-6,-7,-16,-19,-20,-21,-22,-23,-27,-26,83,-25,]),'RPAREN':([13,14,18,19,21,22,23,24,25,37,38,39,42,43,44,45,46,47,48,49,50,51,52,53,54,55,58,59,63,79,],[-47,-42,-40,-41,40,-11,-12,-14,-47,-38,55,-15,59,-45,-46,-28,-29,-30,-31,-32,-33,-34,-35,-36,-37,-39,-13,-43,-44,81,]),'PLUS':([14,15,18,19,37,38,43,45,46,47,48,49,50,51,52,53,54,55,59,63,71,78,79,],[-42,27,-40,-41,-38,27,27,-28,-29,-30,-31,27,27,27,27,27,27,-39,-43,27,27,27,27,]),'MULTIPLY':([14,15,18,19,37,38,43,45,46,47,48,49,50,51,52,53,54,55,59,63,71,78,79,],[-42,29,-40,-41,-38,29,29,29,29,-30,-31,29,29,29,29,29,29,-39,-43,29,29,29,29,]),'DIVIDE':([14,15,18,19,37,38,43,45,46,47,48,49,50,51,52,53,54,55,59,63,71,78,79,],[-42,30,-40,-41,-38,30,30,30,30,-30,-31,30,30,30,30,30,30,-39,-43,30,30,30,30,]),'LT':([14,15,18,19,37,38,43,45,46,47,48,49,50,51,52,53,54,55,59,63,71,78,79,],[-42,31,-40,-41,-38,31,31,-28,-29,-30,-31,-32,-33,-34,-35,31,31,-39,-43,31,31,31,31,]),'GT':([14,15,18,19,37,38,43,45,46,47,48,49,50,51,52,53,54,55,59,63,71,78,79,],[-42,32,-40,-41,-38,32,32,-28,-29,-30,-31,-32,-33,-34,-35,32,32,-39,-43,32,32,32,32,]),'LE':([14,15,18,19,37,38,43,45,46,47,48,49,50,51,52,53,54,55,59,63,71,78,79,],[-42,33,-40,-41,-38,33,33,-28,-29,-30,-31,-32,-33,-34,-35,33,33,-39,-43,33,33,33,33,]),'GE':([14,15,18,19,37,38,43,45,46,47,48,49,50,51,52,53,54,55,59,63,71,78,79,],[-42,34,-40,-41,-38,34,34,-28,-29,-30,-31,-32,-33,-34,-35,34,34,-39,-43,34,34,34,34,]),'EQ':([14,15,18,19,37,38,43,45,46,47,48,49,50,51,52,53,54,55,59,63,71,78,79,],[-42,35,-40,-41,-38,35,35,-28,-29,-30,-31,-32,-33,-34,-35,-36,-37,-39,-43,35,35,35,35,]),'NE':([14,15,18,19,37,38,43,45,46,47,48,49,50,51,52,53,54,55,59,63,71,78,79,],[-42,36,-40,-41,-38,36,36,-28,-29,-30,-31,-32,-33,-34,-35,-36,-37,-39,-43,36,36,36,36,]),'COMMA':([14,18,19,22,24,25,37,39,42,43,44,45,46,47,48,49,50,51,52,53,54,55,58,59,63,],[-42,-40,-41,41,-14,-47,-38,-15,60,-45,-46,-28,-29,-30,-31,-32,-33,-34,-35,-36,-37,-39,-13,-43,-44,]),}

_lr_action = {}
for _k, _v in _lr_action_items.items():
   for _x,_y in zip(_v[0],_v[1]):
      if not _x in _lr_action:  _lr_action[_x] = {}
      _lr_action[_x][_k] = _y
del _lr_action_items

_lr_goto_items = {'program':([0,],[1,]),'declarations':([0,],[2,]),'declaration':([0,2,],[3,9,]),'var_declaration':([0,2,61,81,83,],[4,4,67,67,67,]),'func_declaration':([0,2,],[5,5,]),'type_specifier':([0,2,13,41,61,81,83,],[6,6,20,20,72,72,72,]),'expression':([12,16,17,25,27,28,29,30,31,32,33,34,35,36,60,61,74,77,81,83,],[15,37,38,43,45,46,47,48,49,50,51,52,53,54,63,71,78,79,71,71,]),'params':([13,],[21,]),'param_list':([13,],[22,]),'empty':([13,25,57,],[23,44,62,]),'param':([13,41,],[24,58,]),'arg_list':([25,],[42,]),'compound_stmt':([40,61,81,83,],[56,68,68,68,]),'stmt_list':([57,],[61,]),'statement':([61,81,83,],[65,82,84,]),'expr_stmt':([61,81,83,],[66,66,66,]),'if_stmt':([61,81,83,],[69,69,69,]),'return_stmt':([61,81,83,],[70,70,70,]),}

_lr_goto = {}
for _k, _v in _lr_goto_items.items():
   for _x, _y in zip(_v[0], _v[1]):
       if not _x in _lr_goto: _lr_goto[_x] = {}
       _lr_goto[_x][_k] = _y
del _lr_goto_items
_lr_productions = [
  ("S' -> program","S'",1,None,None,None),
  ('program -> declarations','program',1,'p_program','parser.py',32),
  ('declarations -> declarations declaration','declarations',2,'p_declarations','parser.py',37),
  ('declarations -> declaration','declarations',1,'p_declarations','parser.py',38),
  ('declaration -> var_declaration','declaration',1,'p_declaration','parser.py',45),
  ('declaration -> func_declaration','declaration',1,'p_declaration','parser.py',46),
  ('var_declaration -> type_specifier IDENTIFIER SEMICOLON','var_declaration',3,'p_var_declaration','parser.py',50),
  ('var_declaration -> type_specifier IDENTIFIER ASSIGN expression SEMICOLON','var_declaration',5,'p_var_declaration','parser.py',51),
  ('type_specifier -> INT','type_specifier',1,'p_type_specifier','parser.py',61),
  ('type_specifier -> FLOAT','type_specifier',1,'p_type_specifier','parser.py',62),
  ('func_declaration -> type_specifier IDENTIFIER LPAREN params RPAREN compound_stmt','func_declaration',6,'p_func_declaration','parser.py',66),
  ('params -> param_list','params',1,'p_params','parser.py',74),
  ('params -> empty','params',1,'p_params','parser.py',75),
  ('param_list -> param_list COMMA param','param_list',3,'p_param_list','parser.py',79),
  ('param_list -> param','param_list',1,'p_param_list','parser.py',80),
  ('param -> type_specifier IDENTIFIER','param',2,'p_param','parser.py',87),
  ('compound_stmt -> LBRACE stmt_list RBRACE','compound_stmt',3,'p_compound_stmt','parser.py',93),
  ('stmt_list -> stmt_list statement','stmt_list',2,'p_stmt_list','parser.py',97),
  ('stmt_list -> empty','stmt_list',1,'p_stmt_list','parser.py',98),
  ('statement -> expr_stmt','statement',1,'p_statement','parser.py',105),
  ('statement -> var_declaration','statement',1,'p_statement','parser.py',106),
  ('statement -> compound_stmt','statement',1,'p_statement','parser.py',107),
  ('statement -> if_stmt','statement',1,'p_statement','parser.py',108),
  ('statement -> return_stmt','statement',1,'p_statement','parser.py',109),
  ('if_stmt -> IF LPAREN expression RPAREN statement','if_stmt',5,'p_if_stmt','parser.py',113),
  ('if_stmt -> IF LPAREN expression RPAREN statement ELSE statement','if_stmt',7,'p_if_stmt','parser.py',114),
  ('return_stmt -> RETURN expression SEMICOLON','return_stmt',3,'p_return_stmt','parser.py',121),
  ('expr_stmt -> expression SEMICOLON','expr_stmt',2,'p_expr_stmt','parser.py',126),
  ('expression -> expression PLUS expression','expression',3,'p_expression_binop','parser.py',130),
  ('expression -> expression MINUS expression','expression',3,'p_expression_binop','parser.py',131),
  ('expression -> expression MULTIPLY expression','expression',3,'p_expression_binop','parser.py',132),
  ('expression -> expression DIVIDE expression','expression',3,'p_expression_binop','parser.py',133),
  ('expression -> expression LT expression','expression',3,'p_expression_binop','parser.py',134),
  ('expression -> expression GT expression','expression',3,'p_expression_binop','parser.py',135),
  ('expression -> expression LE expression','expression',3,'p_expression_binop','parser.py',136),
  ('expression -> expression GE expression','expression',3,'p_expression_binop','parser.py',137),
  ('expression -> expression EQ expression','expression',3,'p_expression_binop','parser.py',138),
  ('expression -> expression NE expression','expression',3,'p_expression_binop','parser.py',139),
  ('expression -> MINUS expression','expression',2,'p_expression_uminus','parser.py',146),
  ('expression -> LPAREN expression RPAREN','expression',3,'p_expression_group','parser.py',151),
  ('expression -> INT_LITERAL','expression',1,'p_expression_number','parser.py',155),
  ('expression -> FLOAT_LITERAL','expression',1,'p_expression_number','parser.py',156),
  ('expression -> IDENTIFIER','expression',1,'p_expression_identifier','parser.py',160),
  ('expression -> IDENTIFIER LPAREN arg_list RPAREN','expression',4,'p_expression_call','parser.py',164),
  ('arg_list -> arg_list COMMA expression','arg_list',3,'p_arg_list','parser.py',168),
  ('arg_list -> expression','arg_list',1,'p_arg_list','parser.py',169),
  ('arg_list -> empty','arg_list',1,'p_arg_list','parser.py',170),
  ('empty -> <empty>','empty',0,'p_empty','parser.py',182),
]
# src/parser/__init__.py

from .parser import parse_tokens
from .ast_nodes import (
    ASTNode,
    Program,
    FunctionDeclaration,
    VariableDeclaration,
    IfStatement,
    ReturnStatement,
    BinaryOperation,
    UnaryOperation,
    Identifier,
    Literal,
    CompoundStatement
)

__all__ = [
    'parse_tokens',
    'ASTNode',
    'Program',
    'FunctionDeclaration',
    'VariableDeclaration',
    'IfStatement',
    'ReturnStatement',
    'BinaryOperation',
    'UnaryOperation',
    'Identifier',
    'Literal',
    'CompoundStatement'
]
# src/utils/ast_visualizer.py

from graphviz import Digraph
from parser.ast_nodes import (
    Program,
    FunctionDeclaration,
    VariableDeclaration,
    IfStatement,
    ReturnStatement,
    BinaryOperation,
    UnaryOperation,
    Identifier,
    Literal,
    CompoundStatement,
    FunctionCall  
)

class ASTVisualizer:
    def __init__(self):
        self.dot = Digraph(comment='Abstract Syntax Tree')
        self.dot.attr('node', shape='box', style='filled', color='lightblue')

    def visualize(self, ast_root, output_filename='ast_output'):
        """
        Traverses the AST and generates a Graphviz graph.

        :param ast_root: The root node of the AST.
        :param output_filename: The base name for the output files (without extension).
        """
        self.visit(ast_root)
        try:
            self.dot.render(filename=output_filename, format='png', cleanup=True)
            print(f"AST visualization saved as {output_filename}.png")
        except Exception as e:
            print(f"Failed to render AST visualization: {e}")

    def visit(self, node, parent=None):
        """
        Recursively visits AST nodes and adds them to the graph.

        :param node: The current AST node.
        :param parent: The parent node in the AST.
        """
        if node is None:
            return

        # Add the current node to the graph
        label = node.get_label()
        self.dot.node(str(node.id), label)

        # If there's a parent, add an edge from parent to current node
        if parent is not None:
            self.dot.edge(str(parent.id), str(node.id))

        # Recursively visit child nodes based on node type
        if isinstance(node, Program):
            for decl in node.declarations:
                self.visit(decl, node)

        elif isinstance(node, FunctionDeclaration):
            for param in node.params:
                self.visit(param, node)
            self.visit(node.body, node)

        elif isinstance(node, VariableDeclaration):
            if node.initializer:
                self.visit(node.initializer, node)

        elif isinstance(node, IfStatement):
            self.visit(node.condition, node)
            self.visit(node.then_branch, node)
            if node.else_branch:
                self.visit(node.else_branch, node)

        elif isinstance(node, ReturnStatement):
            self.visit(node.expression, node)

        elif isinstance(node, BinaryOperation):
            self.visit(node.left, node)
            self.visit(node.right, node)

        elif isinstance(node, UnaryOperation):
            self.visit(node.operand, node)

        elif isinstance(node, FunctionCall):  # Add handling for FunctionCall
            self.visit(node.name, node)
            for arg in node.args:
                self.visit(arg, node)

        elif isinstance(node, Identifier):
            pass  # Leaf node

        elif isinstance(node, Literal):
            pass  # Leaf node

        elif isinstance(node, CompoundStatement):
            for stmt in node.statements:
                self.visit(stmt, node)

        else:
            print(f"Unhandled node type: {type(node).__name__}")
# src/utils/__init__.py

from .ast_visualizer import ASTVisualizer

__all__ = ['ASTVisualizer']
